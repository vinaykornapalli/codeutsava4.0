{"cells":[{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import pickle\ndef load_data(filename):\n    data = pickle.load(open(filename, 'rb'))\n    x_text = []\n    labels = []\n    for i in range(len(data)):\n        if(HASH_REMOVE):\n            x_text.append(p.tokenize((data[i]['text']).encode('utf-8')))\n        else:\n            x_text.append(data[i]['text'])\n        labels.append(data[i]['label'])\n    return x_text,labels\n\ndef get_filename(dataset):\n    global N_CLASS, HASH_REMOVE\n    if(dataset==\"twitter\"):\n        filename = \"../input/formspring1/twitter_data.pkl\"\n        N_CLASS = 3\n        HASH_REMOVE = False\n    elif(dataset==\"formspring\"):\n        N_CLASS = 2\n        filename = \"../input/formspring_data.pkl\"\n        HASH_REMOVE = False\n    elif(dataset==\"wiki\"):\n        N_CLASS = 2\n        filename = \"../input/formspring1/wiki_data.pkl\"\n        HASH_REMOVE = False\n    return filename\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = \"formspring\"\nx_text, labels = load_data(get_filename(data)) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_corp = []\nfor x in range(len(x_text)):\n    words = re.split(r'\\W+', x_text[x])\n    words = [word.lower() for word in words]\n    x_corp.append(words)\n    \nx_corp = [[' '.join(i)] for i in x_corp]","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"","_uuid":"","trusted":true},"cell_type":"code","source":"import pandas as pd\n\nfor x in range(len(labels)):\n    if labels[x] ==0 :\n        labels[x] = 1\n    else:\n        labels[x] = 2\n\n\ndata = pd.concat([pd.DataFrame(x_corp),pd.DataFrame(labels)],axis=1)\n\ndata.columns = ['text', 'label']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"count=0\nfinal = []\nfor x in range(len(x_corp)):\n    if data.iloc[x,0] == ''or data.iloc[x,0] == ' ' or data.iloc[x,1] == '' or data.iloc[x,1] == ' ':\n        count+=1\n        continue\n    final.append(data.iloc[x,:].values)\n       \nprint(count)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"!pip install tiny-tokenizer\n!pip install flair","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding = DocumentPoolEmbeddings([WordEmbeddings('glove'),ELMoEmbeddings()], fine_tune_mode='nonlinear')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nfrom flair.data import Sentence\nfor i in  range(len(final)):\n    sentence = final[i][0]\n    sentence = Sentence(sentence)\n    embedding.embed(sentence)\n    final[i][0] = sentence.embedding.tolist()\n    os.system( 'cls' )\n    print(i)\n    os.system( 'cls' )\n   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nfrom keras import models\nfrom keras.layers import Dense,Conv1D,Dropout,Flatten,Dense,MaxPooling1D\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import StratifiedKFold\nnp.random.seed(0)\n\ndef create_network():\n    \n    model = models.Sequential()\n    model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(3172,1)))\n    model.add(Dropout(0.5))\n    model.add(MaxPooling1D())\n    model.add(Flatten())\n    model.add(Dense(units=1, activation='sigmoid'))\n    model.compile(loss='binary_crossentropy',optimizer='rmsprop',metrics=['accuracy'])\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"neural_network = KerasClassifier(build_fn=create_network, epochs=200, batch_size=100, verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=0)\nresults = cross_val_score(neural_network, X, y, cv=10)\nprint(results.mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = np.vstack(final[:,0])\ny = np.vstack(final[:,1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = np.reshape(X, (X.shape[0], 1, X.shape[1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = X.reshape(12759,3172,1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final.to_csv('embeddings.csv',index=None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\ntrain, validate, test = np.split(final.sample(frac=1), [int(.7*len(final)), int(.85*len(final))])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.to_csv('train.csv',index=None)\nvalidate.to_csv('dev.csv',index=None)\ntest.to_csv('test.csv',index=None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install tiny-tokenizer\n!pip install flair","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\ndataset = pd.read_csv('/kaggle/working/train.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"count=0\nfinal = []\nfor x in range(len(dataset)):\n    if dataset.iloc[x,0] != ''or dataset.iloc[x,0] != ' ' or dataset.iloc[x,1] != ''or dataset.iloc[x,1] != ' ':\n        final.append(dataset.iloc[x,:].values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(dataset)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from flair.data import Corpus\ndata_folder ='/kaggle/working'\ncolumn_name_map = {0: \"text\",1: \"label_topic\"}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from flair.data import Sentence,FlairDataset\nfrom flair.data import Token, segtok_tokenizer\nfrom pathlib import Path\nfrom typing import List, Dict, Union, Callable\nimport logging\nimport csv\n\nclass CSVClassificationData(FlairDataset):\n    def __init__(\n        self,\n        path_to_file: Union[str, Path],\n        column_name_map: Dict[int, str],\n        max_tokens_per_doc: int = -1,\n        max_chars_per_doc: int = -1,\n        tokenizer=segtok_tokenizer,\n        in_memory: bool = True,\n        skip_header: bool = False,\n        **fmtparams,\n    ):\n    \n\n        if type(path_to_file) == str:\n            path_to_file: Path = Path(path_to_file)\n\n        assert path_to_file.exists()\n\n        # variables\n        self.path_to_file = path_to_file\n        self.in_memory = in_memory\n        self.tokenizer = tokenizer\n        self.column_name_map = column_name_map\n        self.max_tokens_per_doc = max_tokens_per_doc\n        self.max_chars_per_doc = max_chars_per_doc\n\n        # different handling of in_memory data than streaming data\n        if self.in_memory:\n            self.sentences = []\n        else:\n            self.raw_data = []\n\n        self.total_sentence_count: int = 0\n\n        # most data sets have the token text in the first column, if not, pass 'text' as column\n        self.text_columns: List[int] = []\n        for column in column_name_map:\n            if column_name_map[column] == \"text\":\n                self.text_columns.append(column)\n\n        with open(self.path_to_file) as csv_file:\n\n            csv_reader = csv.reader(csv_file, **fmtparams)\n\n            if skip_header:\n                next(csv_reader, None)  # skip the headers\n\n            for row in csv_reader:\n                # test if format is OK\n                wrong_format = False\n                for text_column in self.text_columns:\n                    if text_column >= len(row):\n                        wrong_format = True\n\n                if wrong_format:\n                    continue\n                \n\n                # test if at least one label given\n                has_label = False\n                for column in self.column_name_map:\n                    if self.column_name_map[column].startswith(\"label\") and row[column]:\n                        has_label = True\n                        break\n                \n\n                if not has_label:\n                    continue\n\n                if self.in_memory:\n                    \n                    print('yes')\n\n                    text = \" \".join(\n                        [row[text_column] for text_column in self.text_columns]\n                    )\n                    \n                    if self.max_chars_per_doc > 0:\n                        text = text[: self.max_chars_per_doc]\n\n                    sentence = Sentence(text, use_tokenizer=self.tokenizer)\n\n                    for column in self.column_name_map:\n                        if (\n                            self.column_name_map[column].startswith(\"label\")\n                            and row[column]\n                        ):\n                            sentence.add_label(row[column])\n\n                    if 0 < self.max_tokens_per_doc < len(sentence):\n                        sentence.tokens = sentence.tokens[: self.max_tokens_per_doc]\n                    self.sentences.append(sentence)\n\n                else:\n                    print('no')\n                    self.raw_data.append(row)\n\n                self.total_sentence_count += 1\n                print(self.total_sentence_count)\n\n    def is_in_memory(self) -> bool:\n        return self.in_memory\n\n    def __len__(self):\n        return self.total_sentence_count\n\n    def __getitem__(self, index: int = 0) -> Sentence:\n        if self.in_memory:\n            return self.sentences[index]\n        else:\n            row = self.raw_data[index]\n\n            text = \" \".join([row[text_column] for text_column in self.text_columns])\n\n            if self.max_chars_per_doc > 0:\n                text = text[: self.max_chars_per_doc]\n\n            sentence = Sentence(text, use_tokenizer=self.tokenizer)\n            for column in self.column_name_map:\n                if self.column_name_map[column].startswith(\"label\") and row[column]:\n                    sentence.add_label(row[column])\n\n            if 0 < self.max_tokens_per_doc < len(sentence):\n                sentence.tokens = sentence.tokens[: self.max_tokens_per_doc]\n\n            return sentence\n\nlog = logging.getLogger(\"flair\")\n\nclass CSVClassificationCorp(Corpus):\n    def __init__(\n        self,\n        data_folder: Union[str, Path],\n        column_name_map: Dict[int, str],\n        train_file=None,\n        test_file=None,\n        dev_file=None,\n        tokenizer: Callable[[str], List[Token]] = segtok_tokenizer,\n        max_tokens_per_doc=-1,\n        max_chars_per_doc=-1,\n        in_memory: bool = False,\n        skip_header: bool = False,\n        **fmtparams,\n    ):\n    \n\n        if type(data_folder) == str:\n            data_folder: Path = Path(data_folder)\n\n        if train_file is not None:\n            train_file = data_folder / train_file\n        if test_file is not None:\n            test_file = data_folder / test_file\n        if dev_file is not None:\n            dev_file = data_folder / dev_file\n\n        # automatically identify train / test / dev files\n        if train_file is None:\n            for file in data_folder.iterdir():\n                file_name = file.name\n                if \"train\" in file_name:\n                    train_file = file\n                if \"test\" in file_name:\n                    test_file = file\n                if \"dev\" in file_name:\n                    dev_file = file\n                if \"testa\" in file_name:\n                    dev_file = file\n                if \"testb\" in file_name:\n                    test_file = file\n\n        log.info(\"Reading data from {}\".format(data_folder))\n        log.info(\"Train: {}\".format(train_file))\n        log.info(\"Dev: {}\".format(dev_file))\n        log.info(\"Test: {}\".format(test_file))\n\n        train: Dataset = CSVClassificationData(\n            train_file,\n            column_name_map,\n            tokenizer=tokenizer,\n            max_tokens_per_doc=max_tokens_per_doc,\n            max_chars_per_doc=max_chars_per_doc,\n            in_memory=in_memory,\n            skip_header=skip_header,\n            **fmtparams,\n        )\n\n        if test_file is not None:\n            test: Dataset = CSVClassificationData(\n                test_file,\n                column_name_map,\n                tokenizer=tokenizer,\n                max_tokens_per_doc=max_tokens_per_doc,\n                max_chars_per_doc=max_chars_per_doc,\n                in_memory=in_memory,\n                skip_header=skip_header,\n                **fmtparams,\n            )\n        else:\n            train_length = len(train)\n            test_size: int = round(train_length / 10)\n            splits = random_split(train, [train_length - test_size, test_size])\n            train = splits[0]\n            test = splits[1]\n\n        if dev_file is not None:\n            dev: Dataset = CSVClassificationData(\n                dev_file,\n                column_name_map,\n                tokenizer=tokenizer,\n                max_tokens_per_doc=max_tokens_per_doc,\n                max_chars_per_doc=max_chars_per_doc,\n                in_memory=in_memory,\n                skip_header=skip_header,\n                **fmtparams,\n            )\n        else:\n            train_length = len(train)\n            dev_size: int = round(train_length / 10)\n            splits = random_split(train, [train_length - dev_size, dev_size])\n            train = splits[0]\n            dev = splits[1]\n\n        super(CSVClassificationCorp, self).__init__(\n            train, dev, test, name=data_folder.name\n        )\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corpus: Corpus = CSVClassificationCorp(data_folder,\n                                         column_name_map,\n                                         skip_header=True,\n                                         delimiter=',', \n) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(corpus)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"label_dict = corpus.make_label_dictionary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from flair.embeddings import WordEmbeddings, ELMoEmbeddings, FlairEmbeddings,DocumentRNNEmbeddings,DocumentPoolEmbeddings\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true,"_kg_hide-input":true},"cell_type":"code","source":"from flair.models import TextClassifier\nfrom flair.trainers import ModelTrainer\ndocument_embeddings: document_embeddings = DocumentPoolEmbeddings([WordEmbeddings('glove'),ELMoEmbeddings()], fine_tune_mode='nonlinear')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"classifier = TextClassifier(document_embeddings, label_dictionary=label_dict)\n\ntrainer = ModelTrainer(classifier, corpus)\n\ntrainer.train('resources/taggers/ag_news',\n              learning_rate=0.05,\n              mini_batch_size=25,\n              anneal_factor=0.05,\n              patience=3,\n              max_epochs=100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sentence = Sentence('you are a whore')\n\n# predict class and print\nclassifier.predict(sentence)\n\nprint(sentence.labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nos.chdir(r'/kaggle/working')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from IPython.display import FileLink\nFileLink(r'train.pickle')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\ndataset = pd.read_csv('embeddings.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"trusted":true},"cell_type":"code","source":"\nwith open('train.pickle', 'wb') as f:\n    pickle.dump([X,y], f)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"}},"nbformat":4,"nbformat_minor":1}